{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7592722d",
   "metadata": {},
   "source": [
    "# Click \"Edit App\" to see the code\n",
    "# Histogram and normal distribution\n",
    "\n",
    "In this tutorial we'll learn how to read a CSV file into a _pands_ DataFrame, compute the average of the data in the second column, build a histogram and compare it to the _normal_ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9536a",
   "metadata": {},
   "source": [
    "# The Jupyter Notebook\n",
    "Let's start by loading the usual Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages\n",
    "import pandas as pd # Dataframes and reading CSV files\n",
    "import numpy as np # Numerical libraries\n",
    "import matplotlib.pyplot as plt # Plotting library\n",
    "from lmfit import Model # Least squares fitting library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11751d",
   "metadata": {},
   "source": [
    "We can now create a _pandas_ DataFrame_ that contains the data we generated previously.\n",
    "We can use the pandas' function **read_csv** to store the file and store its content in a dataframe. We can also create two DataFrames, one for each of the two Random number functions that were in the first week's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b563d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"../miscData/random1.csv\")\n",
    "data2 = pd.read_csv(\"../miscData/random2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecdf24",
   "metadata": {},
   "source": [
    "We can then print the dataframe to see what it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2662a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First DataFrame\")\n",
    "print(data1)\n",
    "print(\"Second DataFrame\")\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0e539",
   "metadata": {},
   "source": [
    "For simplicity, we can change the names of the columns of the DataFrames to be the same.\n",
    "This could be useful for referencing the content of the DataFrames later.\n",
    "We don't need to print the entire DataFrame again, but we can just check that the headers have changed, and are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns = (\"X\",\"Y\")\n",
    "data2.columns = (\"X\",\"Y\")\n",
    "print(data1.columns)\n",
    "print(data2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49e8b1",
   "metadata": {},
   "source": [
    "One useful information about the DataFrame that we normally need to know is the number of lines it contains.\n",
    "This can be achieved _e.g_ by _measuring_ the length of one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfValues1 = len(data1[\"Y\"])\n",
    "numberOfValues2 = len(data2[\"Y\"])\n",
    "print(\"Number of data points in the first file :\",numberOfValues1)\n",
    "print(\"Number of data points in the second file :\",numberOfValues2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4d8ea",
   "metadata": {},
   "source": [
    "Let's now compute the average of the data in the second column.\n",
    "The first column is just an index. The simplest way to do this is to use the NumPy function _mean_.\n",
    "There are multiple ways of selecting the data in the second column of the dataframe.\n",
    "Here we use the *name* of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faff9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "average1 = np.mean(data1[\"Y\"])\n",
    "average2 = np.mean(data2[\"Y\"])\n",
    "print(\"Averages, method #1 :\",average1, average2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015885d",
   "metadata": {},
   "source": [
    "Alternatively, we can use the **iloc** function, which allows us to specify the desired range of the data that we want to look at.\n",
    "* Remember that python starts counting from zero and that the upper limit of the range is not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81161547",
   "metadata": {},
   "outputs": [],
   "source": [
    "average1 = np.mean(data1.iloc[0:numberOfValues1,1])\n",
    "average2 = np.mean(data2.iloc[0:numberOfValues2,1])\n",
    "print(\"Averages, method #2 :\",average1,average2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59ebbd",
   "metadata": {},
   "source": [
    "Although this is more complicated, we can also compute the average manually using a **for** loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "average1 = 0\n",
    "for val in data1[\"Y\"]:\n",
    "    average1 += val\n",
    "average1 /= numberOfValues1\n",
    "\n",
    "average2 = 0\n",
    "for val in data2[\"Y\"]:\n",
    "    average2 += val\n",
    "average2 /= numberOfValues2\n",
    "\n",
    "print(\"Averages, method #3 :\",average1,average2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63246aa",
   "metadata": {},
   "source": [
    "Analogously, the standard deviation can be readily computed using the NumPy function _std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardDeviation1 = np.std(data1[\"Y\"])\n",
    "standardDeviation2 = np.std(data2[\"Y\"])\n",
    "\n",
    "print(\"Standard deviations :\",standardDeviation1,standardDeviation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b9e9b",
   "metadata": {},
   "source": [
    "Unfortunately, there is no NumPy function for the standard error, so we have to use the definition\n",
    "\\begin{equation}\n",
    "StdErr = \\frac{\\sigma}{\\sqrt{N}}\n",
    "\\end{equation}\n",
    "where $\\sigma$ is the standard deviation and $N$ is the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardError1 = np.std(data2[\"Y\"])  / np.sqrt(numberOfValues1)\n",
    "standardError2 = np.std(data2[\"Y\"])  / np.sqrt(numberOfValues2)\n",
    "print(\"Standard errors :\",standardError1,standardError2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd9422",
   "metadata": {},
   "source": [
    "Let's now compute the histogram of the data, and compare it with the \"normal\" distribution that you have seen in statistics. If our data obey the normal distribution, the \"normalised\" hystogram should resemble a gaussian function centred on the average of the data, whose width is given by the standard deviation of the data.\n",
    "\n",
    "In order to compute the histogram we can use the function **histogram** in NumPy.\n",
    "This function produces two arrays in output, one with the position of the bins\n",
    "and one with the hight of the bar of the histogram.\n",
    "In the example below we compute the histogram using 50 and 75 bins.\n",
    "* Note that the \"bins\" arrays have one extra element. This is because they specify the left and right side of the bin, not its centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0cea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram1 , bins1 = np.histogram(data1[\"Y\"],bins=50)\n",
    "histogram2 , bins2 = np.histogram(data2[\"Y\"],bins=75)\n",
    "print(\"Size of the histogram arrays :\",len(histogram1),len(histogram2))\n",
    "print(\"Size of the bins arrays :\",len(bins1),len(bins2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f6030",
   "metadata": {},
   "source": [
    "The histogram that numpy generates is just the tally of how many data points fall within each bean, it is not a probability. In fact, the integral of a probability must be equal to one\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_{-\\infty}^{+\\infty} P(x)\\ \\mathrm{d}x = 1\n",
    "\\end{equation}\n",
    "\n",
    "while the integral of the histogram, $h(x)$, that we have generated is equal to the number of values times the bins' width, which in this case is constant\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_{-\\infty}^{+\\infty} h(x)\\ \\mathrm{d}x = \\sum_i (h_i\\ \\mathrm{d}x) = N\\ \\mathrm{d}x\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Let's verify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of the heights of the histogram bars :\",\n",
    "      np.sum(histogram1),np.sum(histogram2))\n",
    "dx1 = bins1[1] - bins1[0]\n",
    "dx2 = bins2[1] - bins2[0]\n",
    "integral1 = dx1*np.sum(histogram1)\n",
    "integral2 = dx2*np.sum(histogram2)\n",
    "print(\"Integrals of the histograms :\",integral1,integral2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9c1f4",
   "metadata": {},
   "source": [
    "We can therefore normalise the histograms to become _probabilities_ by dividing the height of each bar by the total area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram1 = histogram1 / integral1\n",
    "histogram2 = histogram2 / integral2\n",
    "integral1 = dx1*np.sum(histogram1)\n",
    "integral2 = dx2*np.sum(histogram2)\n",
    "print(\"Integrals of the histograms :\",integral1,integral2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44e8b6",
   "metadata": {},
   "source": [
    "In order to compare the histogram with the _normal_ distribution we need to define a function that returns the values of a normalised Gaussian.\n",
    "\n",
    "\\begin{equation}\n",
    "G(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg[ -\\frac{(x-x_0)^2}{2\\sigma^2} \\Bigg]\n",
    "\\end{equation}\n",
    "\n",
    "where $x_0$ and $\\sigma$ are the mean and standard deviation of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,x0,std):\n",
    "    return np.exp(-0.5*(x-x0)**2 / std**2) / (std * np.sqrt(2*np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b44b7b",
   "metadata": {},
   "source": [
    "We can now evaluate the function at the positions of the _bins_ and put the values in an array.\n",
    "* Note that the output of the function will have the same size of the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalDistribution1 = gaussian(bins1,average1,standardDeviation1)\n",
    "normalDistribution2 = gaussian(bins2,average2,standardDeviation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8ac49",
   "metadata": {},
   "source": [
    "We now have all the elements we need to make a plot and compare the normalised histograms with the normal distributions.\n",
    "\n",
    "We first have to create an object for the figure and its axes. Because we want to make two figures at once, we can use the **subplots** function of the _mathplotlib_ library with the _1,2_ options, which will generate 2 figures laid on one row and two columns. We can also define the _total_ size of the figure (12,6), which would produce two almost square graphs. We then add labels to the axes and the legend and display the figure.\n",
    "\n",
    "* Note that we now have two set of axes, so _ax_ is a two dimensional array.\n",
    "* Note how we can take all elements of an array bar the last one using **[:-1]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc92638",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure , ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "ax[0].bar(bins1[:-1], histogram1, width=0.1, label=\"Histogram\")\n",
    "ax[0].plot(bins1, normalDistribution1, label=\"Gaussian\", color='red')\n",
    "ax[0].set(xlabel=\"Values\")\n",
    "ax[0].set(ylabel=\"Probability\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].bar(bins2[:-1], histogram2,width=0.1, label=\"Histogram\")\n",
    "ax[1].plot(bins2, normalDistribution2, label=\"Gaussian\", color='red')\n",
    "ax[1].set(xlabel=\"Values\")\n",
    "ax[1].set(ylabel=\"Probability\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# If you want we can save the figure to a file\n",
    "# fig1.savefig(\"fig.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d021f48",
   "metadata": {},
   "source": [
    "Note how the first set of data were uniformly distributed between 6 and 20, hence the standard deviation is not a measure of the uncertainty of the data. On the other hand, the data in the second set are _normally_ distributed, hence the variance is indeed a measure of the uncertainty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
