{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cekComputerLabs as cek\n",
    "# cek.checkGitRepo()\n",
    "from IPython.display import Image\n",
    "\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lmfit import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Statistical Analysis and Linear Regression\n",
    "In the short introductory lecture to this numerical laboratory we have briefly discussed averages and linear regressions. In this first activity we can practice how to compute averages and how to perform (linear) regressions using a jupyter note book and some of the most common Physics/Chemistry equations.\n",
    "\n",
    "## Virtual experiment \n",
    "## [_You can start your first laboratory here_](virtualLaboratory.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary\n",
    "\n",
    "- **Analytic solution**:\\\n",
    "Solving a problem with \"pen and paper\" and showing all the steps of the solution.\n",
    "- **Numerical solution**:\\\n",
    "Solving a problem with an algorithm implemented in python or Excel to obtain an approximated solution of the problem.\n",
    "- **Linearise an equation**:\\\n",
    "Rearrange an equation so that after a simple change of variables the plot of the data will be a straight line _e.g._ the Arrhenius equation can be written in either of these two forms\n",
    "\\begin{equation}\n",
    "k = Ae^{-E/T} \\qquad\\qquad \\ln k = -\\frac{E}{RT} + \\ln A\n",
    "\\end{equation}\n",
    "if we then define $y=\\ln K$ and $x=1/T$ we obtain\n",
    "\\begin{equation}\n",
    "y = -\\frac{E}{R} x + \\ln A \\qquad\\qquad y=mx+q\n",
    "\\end{equation}\n",
    "Hence, the slope a linear fit of $y$ _vs_ $x$ is  $m=-E/R$ and the y-axis intercept is  $q=\\ln A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of significant figures\n",
    "\n",
    "When you conduct a measurement, the precision of the value you record corresponds to the measurement's inherent error. For instance, if you specify an object's length as 6.725 cm, it implies an approximate uncertainty of 0.005 cm. Expressing this measurement as either 6.7 or 6.7250012 would suggest that you possess knowledge to the nearest 0.1 cm in the former case or to an extraordinary precision of 0.0000002 cm in the latter. It is advisable to report only as many significant figures as align with the estimated error. In the case of 6.725 cm, it would be considered a four significant figure value, meaning that four digits hold significance in relation to the measurement. It's important to note that this concept isn't dependent on the \"number of decimal places.\" Converting the same measurement to millimeters, it becomes 672.5 mm while still maintaining four significant figures. \n",
    "\n",
    "As per the widely accepted convention, only one uncertain digit should be included in a measurement report. For instance, if the estimated error is 0.03 cm, the result should be presented as 6.7 cm ± 0.03 cm, rather than 6.725 cm ± 0.03 cm.However, quantities should only be reported to a number of significant figures such that their uncertainties has one significant figure, _e.g_\n",
    "* 123 $\\pm$ 1\n",
    "* 123.4 $\\pm$ 0.1\n",
    "* 120 $\\pm$ 10\n",
    "* 120 $\\pm$ 2\n",
    "\n",
    "Any ambiguity about whether the zero is significant or not in the last example is removed when the uncertainty of the measurement is explicitly reported.\n",
    "\n",
    "An advantage of using a notebook or Excel for all the calculations is that you can use variables/cells to store  the results of all the operations up to the machine precision and then round them only the final answer for reporting purposes. If all operations are done using variables, the accuracy of calculations increases since all operations are done in double precision, and the results of such operations are also stored with 32 significant figures.\n",
    "\n",
    "While rounding numbers to a given number of decimal places can be trivially done in python using formatted printing, it is less obvious how to round to the nearest 10 or 100.\n",
    "In order to do that we can use the ```round``` function\n",
    "\n",
    "```\n",
    "round(number, ndigits=None)\n",
    "Return number rounded to ndigits precision after the decimal point. \n",
    "If ndigits is omitted or is None, it returns the nearest integer to its input.\n",
    "\n",
    "For the built-in types supporting round(), values are rounded to the closest \n",
    "multiple of 10 to the power minus ndigits; if two multiples are equally close, \n",
    "rounding is done toward the even choice (so, for example, both round(0.5) and \n",
    "round(-0.5) are 0, and round(1.5) is 2). Any integer value is valid for ndigits \n",
    "(positive, zero, or negative). The return value is an integer if ndigits \n",
    "is omitted or None. Otherwise, the return value has the same type as number.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "For values exactly halfway between rounded decimal values, Python rounds to \n",
    "the nearest even value. Thus 1.5 and 2.5 round to 2.0, -0.5 and 0.5 round to 0.0, \n",
    "etc. Results may also be surprising due to the inexact representation of decimal \n",
    "fractions in the IEEE floating point standard [1]_ and errors introduced when \n",
    "scaling by powers of ten.\n",
    "```\n",
    "\n",
    "Instead, if you want to always round .5 up you can use\n",
    "```python\n",
    "np.rint(np.nextafter(a, a+1))\n",
    "```\n",
    "where ```a``` is your variable.\n",
    "\n",
    "This is however largely immaterial, since a floating point operation will never give exactly a result of .5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 123.456789\n",
    "\n",
    "print(\"Value {:f}\".format(number))\n",
    "print(\"Value {:.4f}\".format(number))\n",
    "print(\"Value {:.2f}\".format(number))\n",
    "print(\"Value {:.0f}\".format(number))\n",
    "print(\"-------\")\n",
    "rounded = round(number,6)\n",
    "print(\"Value {}\".format(rounded))\n",
    "rounded = round(number,4)\n",
    "print(\"Value {}\".format(rounded))\n",
    "rounded = round(number,2)\n",
    "print(\"Value {}\".format(rounded))\n",
    "rounded = round(number,0)\n",
    "print(\"Value {}\".format(rounded))\n",
    "rounded = round(number,-1)\n",
    "print(\"Value {}\".format(rounded))\n",
    "rounded = round(number,-2)\n",
    "print(\"Value {}\".format(rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Errors\n",
    "\n",
    "Random errors stem from the fluctuations observed when conducting multiple trials of a particular measurement. For instance, if you were to repeatedly measure the period of a pendulum using a stopwatch, you would notice that the measurements vary each time. These fluctuations primarily arise from the challenge of precisely determining when the pendulum reaches a specific point in its motion and accurately starting and stopping the stopwatch accordingly. Since you would obtain different period values in each attempt, your result inherently carries uncertainty. In any experiment there are various common sources of such random uncertainties:\n",
    "\n",
    "1. Constraints imposed by the precision of your measuring equipment and the uncertainty in interpolating between the smallest divisions. Precision refers to the smallest directly measurable unit. A typical meter stick, for instance, is divided into millimeters, making its precision one millimeter.\n",
    "\n",
    "2. Unpredictable variations in initial conditions during measurements. \n",
    "\n",
    "3. Absence of a precise definition for the measured quantity. The quantity being measured might be influenced by an uncontrolled variable, such as the temperature of the object.\n",
    "\n",
    "4. Occasionally, the measured quantity is well-defined but is subject to inherent random fluctuations. These fluctuations may have a quantum origin or result from the fact that the values of the measured quantity are determined by the statistical behavior of a large number of particles. For example, AC noise can cause the needle of a voltmeter to fluctuate.\n",
    "\n",
    "Regardless of the source of uncertainty, for it to be classified as \"random,\" the fluctuations from a \"true\" value must be equally likely to be positive or negative. This characteristic provides a crucial insight into how to address random errors. By conducting a large number of measurements and calculating the average result, you can expect that, if uncertainties are truly equally likely to be positive or negative, the average of these measurements will closely approximate the correct value of the measured quantity. This is because positive and negative fluctuations tend to balance each other out over a large number of trials. \n",
    "In this case the measurements are \"normally\" distributed, which means that the normalised histogram of an infinitely large number of observations will be a Gaussian (bell curve) of width $\\sigma$ and centered at the \"true\" value, $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"normal.png\",width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1.** Normal distribution representation By Ainali - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=3141713\n",
    "\n",
    "If the exact values of $\\mu$ and $\\sigma$ are known, a single measurement of the quantity would have \n",
    "* 68.2% change of being within $\\sigma$ from $\\mu$\n",
    "* 95.4% change of being within $2\\sigma$ from $\\mu$\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence interval\n",
    "https://en.wikipedia.org/wiki/Student%27s_t-distribution\n",
    "\n",
    "Because of the true value of a quantity is generally unknown, and the magnitude of the random errors depend on the experimental setup, operator's skills, etc. it important to report our results with confidence intervals. \n",
    "Like saying, \"I am 95% confident that the true value of this quantity sits within this range of values...\"\n",
    "\n",
    "Given a set of $N$ measurements of a certain quantity, ${X} = {x_1, x_2, \\dots, n_N}$, define the average of that quantity as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu = \\frac{\\sum_i x_i}{N},\n",
    "\\end{equation}\n",
    "\n",
    "its standard deviation as\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2 = \\frac{\\sum_i (x_i-\\mu)^2}{N-1},\n",
    "\\end{equation}\n",
    "\n",
    "and its standard error as\n",
    "\\begin{equation}\n",
    "s = \\frac{\\sigma}{\\sqrt{N}},\n",
    "\\end{equation}\n",
    "\n",
    "Because the number observations is usually limited (<50) the standard deviation would provide an underestimate of the confidence interval. \n",
    "A better estimate of the confidence interval for our observable is\n",
    "\n",
    "\\begin{equation}\n",
    "CI = \\mu \\pm ts = \\mu\\pm t\\frac{\\sigma}{\\sqrt{N}}\n",
    "\\end{equation}\n",
    "\n",
    "where $t$ depends on the number of degrees of freedom of our sample, $df=N-1$, and it can be easily obtained from the _student-t_ tables, such as the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"t-table.png\",width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a practical example to see how this works.\n",
    "Imagine we measure a reaction rate using 5 nominally equivalent initial conditions and we obtain these values:\n",
    "\n",
    "| #   | $k_r$    |\n",
    "|:---:|:--------:|\n",
    "|  1  |  0.0291  |\n",
    "|  2  |  0.0321  |\n",
    "|  3  |  0.0293  |\n",
    "|  4  |  0.0295  |\n",
    "|  5  |  0.0272  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the values into a numpy array\n",
    "kr = np.array([0.0291, 0.0321, 0.0293, 0.0295, 0.0272])\n",
    "\n",
    "ndata = len(kr)\n",
    "print(\"The number of values is {}\".format(ndata))\n",
    "\n",
    "ndf = len(kr)-1\n",
    "print(\"The number of degrees of freedom is {}\".format(ndf))\n",
    "\n",
    "mean = np.mean(kr)\n",
    "print(\"The mean is {}\".format(mean))\n",
    "\n",
    "stdev = np.std(kr,ddof=1)\n",
    "print(\"The standard deviation is {}\".format(stdev))\n",
    "\n",
    "sterr = stdev / np.sqrt(ndata)\n",
    "print(\"The standard error is {}\".format(sterr))\n",
    "\n",
    "# Extract the t value from scipy for a two tailed distribution\n",
    "tvalue = stats.t.ppf(q=1-.05/2,df=ndf)\n",
    "print(\"The t value for a 95% confidence interval with {} degrees of freedom is {}\".format(ndf,tvalue))\n",
    "\n",
    "CI = tvalue * sterr\n",
    "print(\"The value of kr is {:.3f} +/- {:.3f}\".format(mean,CI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with experimental data - Student's t test\n",
    "https://en.wikipedia.org/wiki/Student%27s_t-test\n",
    "\n",
    "Once we have calculated our best estimate for a quantity and its 95% confidence interval, we can then compare it with its known/expected value, which we indicate as $k_{exp}$.\n",
    "This can be done by performing the so called _t test_.\n",
    "\n",
    "\\begin{equation}\n",
    "T_{test} = \\frac{\\mu - k_{exp}}{\\sigma\\big/\\sqrt{N}}\n",
    "\\end{equation}\n",
    "\n",
    "The value of $T_{test}$ computed above, should then be compared with the $t$ value obtained before when we computed the confidence interval. If\n",
    "* $T_{test} \\ge -t \\quad or \\quad T_{test} \\le t$ the literature value is within our confidence interval, hence our estimate for $x$ is consistent with the literature value $\\mu$\n",
    "* $T_{test} < -t \\quad or \\quad T_{test}>t$ the literature value is outside our confidence interval, hence our estimate for $x$ is statistically different from the literature value, _i.e._ they are inconsistent.\n",
    "\n",
    "Using the data set above, we can whether our measurement is consistent with the two available literature value of $\\mu_1 = 0.026$ and of $\\mu_2=0.031$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literature = [0.026,0.031]\n",
    "\n",
    "for mu in literature:\n",
    "    Ttest1 = (mean - mu)/sterr\n",
    "    print(\"Ttest = {:.4f} || t value {:.4f}\".format(Ttest1,tvalue))\n",
    "    \n",
    "    if Ttest1 >= -tvalue and Ttest1 <= tvalue:\n",
    "        print(\"My measurement is consitent with a literature value of {:.4f}\\n\".format(mu))\n",
    "    else:\n",
    "        print(\"My measurement is inconsitent with a literature value of {:.4f}\\n\".format(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation of the uncertainty\n",
    "https://en.wikipedia.org/wiki/Propagation_of_uncertainty\n",
    "\n",
    "In statistics, propagation of uncertainty (or propagation of error) is the effect of variables' uncertainties (or errors, more specifically random errors) on the uncertainty of a function based on them. When the variables are the values of experimental measurements they have uncertainties due to measurement limitations (e.g., instrument precision) which propagate due to the combination of variables in the function.\n",
    "\n",
    "Let's assume we have some quantities $x, y,\\dots$ with given uncertainty $\\sigma_x, \\sigma_y, \\dots$ and that we want to compute the uncertainty on another quantity $\\Gamma$ obtained from them $\\Gamma=f(x,y,\\dots)$. There are standard formulae to propagate the error, which depend on what the function $f$ is, for example.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(x)   &= ax       \\qquad  \\qquad \\sigma_f &= \\lvert a\\rvert\\sigma_x \\\\\n",
    "f(x,y) &= ax+bx    \\qquad  \\qquad \\sigma_f &= \\sqrt{a^2\\sigma_x^2 + b^2\\sigma_y^2} \\\\\n",
    "f(x)   &= ax^b     \\qquad  \\qquad \\sigma_f &= \\Bigg\\lvert \\frac{ax^b\\ b\\sigma_x}{x}\\Bigg\\rvert  \\\\\n",
    "f(x)   &= a\\ln(bx) \\qquad  \\qquad \\sigma_f &= \\Bigg\\lvert a\\frac{\\sigma_x}{x}\\Bigg\\rvert  \\\\\n",
    "f(x,y) &= xy       \\qquad  \\qquad \\sigma_f &= \\lvert f\\rvert\\sqrt{\\Bigg(\\frac{\\sigma_x}{x}\\Bigg)^2 + \\Bigg(\\displaystyle\\frac{\\sigma_y}{y}\\Bigg)^2} \\\\\n",
    "f(x,y) &= x/y       \\qquad  \\qquad \\sigma_f &= \\lvert f\\rvert\\sqrt{\\Bigg(\\frac{\\sigma_x}{x}\\Bigg)^2 + \\Bigg(\\displaystyle\\frac{\\sigma_y}{y}\\Bigg)^2} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $a$ and $b$ are pure numbers with no uncertainty.\n",
    "\n",
    "### Example 1\n",
    "Let's imagine we want to prepare a solution of potassium hydrogen phthalate (molar mass = 204.22 g/mol). Given the precision of the scale and of our pipette we measure\n",
    "* 15.00 $\\pm$ 0.05 g of potassium hydrogen phthalate\n",
    "* 100.0 $\\pm$ 0.1 mL of DI water\n",
    "\n",
    "What is the uncertainty of the concentration of the final solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem data\n",
    "molarMass = 204.22 # g/mol\n",
    "mass = 15.00 # g\n",
    "mass_sigma = 0.05\n",
    "volume = 0.1000 # L\n",
    "volume_sigma = 0.0001\n",
    "\n",
    "# Compute the number of moles and its uncertainty\n",
    "moles = mass / molarMass\n",
    "moles_sigma = mass_sigma / molarMass\n",
    "\n",
    "# Compute the concentration ...\n",
    "conc = moles / volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the concentration is obtained as the ratio of two quantities with uncertainties, we use \n",
    "\\begin{eqnarray}\n",
    "f(x,y) &= x/y       \\qquad  \\qquad \\sigma_f &= \\lvert f\\rvert\\sqrt{\\Bigg(\\frac{\\sigma_x}{x}\\Bigg)^2 + \\Bigg(\\displaystyle\\frac{\\sigma_y}{y}\\Bigg)^2} \\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and its uncertainty\n",
    "conc_sigma = conc * np.sqrt( (moles_sigma/moles)**2 + (volume_sigma/volume)**2 )\n",
    "\n",
    "print(\"Solution concentration = {:.3f} +/- {:.3f} mol/L\".format(conc,conc_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "We prepared two stock solutions of potassium hydrogen phthalate \n",
    "* 0.735 $\\pm$ 0.003\n",
    "* 0.512 $\\pm$ 0.007\n",
    "\n",
    "what is the concentration of potassium hydrogen phthalate if we mix equal amount of the two stock solutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the concentrations and uncertainties in lists\n",
    "solution1 = [0.735 , 0.003] \n",
    "solution2 = [0.512 , 0.007]\n",
    "\n",
    "mix = (solution1[0] + solution2[0]) / 2\n",
    "mix_sigma = np.sqrt(solution1[1]**2 + solution2[1]**2) / 2\n",
    "\n",
    "print(\"Solution concentration = {:.3f} +/- {:.3f} mol/L\".format(mix,mix_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Curve fitting\n",
    "https://en.wikipedia.org/wiki/Least_squares\n",
    "\n",
    "Curve fitting is generally done by minimising the sum of the squares of the residuals (the difference between an observed value and the fitted value provided by a model) by varying the model parameters. If we have a set of observations $y$ at different conditions $x$, we can fit those data using a function that contains a number of parameters $f(x,a,b,c\\dots)$, which has to be smaller than the number of values we have. For example\n",
    "* linear fit:\\\n",
    "$f(x,a,b) = ax + b$\n",
    "\n",
    "* quadratic fit:\\\n",
    "$f(x,a,b,c) = ax^2 + bx + c$\n",
    "\n",
    "* exponential fit:\\\n",
    "$f(x,a,b) = a\\ e^{bx}$\n",
    "\n",
    "* $\\dots$\n",
    "\n",
    "There are many ways of fitting a function in python, the one we would suggest you to use is [_lmfit_](https://lmfit.github.io/lmfit-py/). In the example below, we will demonstrate\n",
    "* How to import data from a CSV file\n",
    "* How to perform a linear fit\n",
    "* How to extract the values of the fitting parameters and their uncertainties\n",
    "* How to show to goodness of the fit\n",
    "* How to use the fitted function to extract the expected an arbitrary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data in a Pandas DataFrame from a file with columns separated by commas\n",
    "data = pd.read_csv(\"../../miscData/LB.csv\")\n",
    "# Import the data from a file with columns separated by white spaces\n",
    "# Skip comments and file has no header\n",
    "#data = pd.read_csv(\"../../miscData/timing.dat\",sep='\\s+',\n",
    "#                   comment=\"#\",header=None)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Change column names\n",
    "data.columns = (\"x\",\"y\",\"e\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity we put the values in the dataframe into variables\n",
    "x = data[\"x\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "# Define the function to be used in the fitting\n",
    "def func(x,a,b):\n",
    "    y = a*x + b\n",
    "    return y\n",
    "\n",
    "# Create the lmfit model\n",
    "lmodel = Model(func)\n",
    "\n",
    "# Initialise the function parameters to be optimised\n",
    "params = lmodel.make_params(a=1,b=1)\n",
    "\n",
    "# Fit the data:\n",
    "#   first argument is y\n",
    "#   second argument are the fitting parameters\n",
    "#   third argument is x\n",
    "result = lmodel.fit(y,params,x=x)\n",
    "\n",
    "# Print the summary of the result\n",
    "print(result.fit_report())\n",
    "\n",
    "# Goodness of the fit\n",
    "fig = result.plot()\n",
    "ax = fig.gca()\n",
    "ax.set(xlabel=\"Y label\")\n",
    "ax.set(ylabel=\"X label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the value of the parameters\n",
    "aParam = result.params[\"a\"].value\n",
    "bParam = result.params[\"b\"].value\n",
    "\n",
    "# Extract the uncertainty of the parameters\n",
    "# Lmfit reports the confidence interval at 1 sigma, hence we need to multiply \n",
    "# the uncertainty by 1.960 to get thr 95% confidence interval\n",
    "aErr = result.params[\"a\"].stderr * 1.960\n",
    "bErr = result.params[\"b\"].stderr * 1.960\n",
    "print(\"The slope is {:.3f} +/- {:.3f}\".format(aParam,aErr))\n",
    "print(\"The slope y-axis intercept is {:.2f} +/- {:.2f}\".format(bParam,bErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct evaluation of the function at a given x value\n",
    "# Note that we have use a numpy array to the result object\n",
    "xValue = np.array([20.])\n",
    "yValue = func(xValue,aParam,bParam)\n",
    "# Here we use the \"*\" operator to expand the list content\n",
    "print(\"The value of func at x={} is {}\".format(xValue,yValue))\n",
    "print(\"The value of func at x={:.2f} is {:.2f}\".format(*xValue,*yValue))\n",
    "\n",
    "# Evaluation of the function using lmfit to get the 95% confidence interval\n",
    "# Note that we have to pass a numpy array to the result object\n",
    "yValue = result.eval(x=xValue)\n",
    "yError = result.eval_uncertainty(x=xValue,sigma=0.95)\n",
    "print(\"The value of func at x={} is {} +/- {}\".format(xValue,yValue,yError))\n",
    "print(\"The value of func at x={} is {:.2f} +/- {:.2f}\".format(*xValue,*yValue,*yError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function evalation for a set of values and plotting with the data\n",
    "#   50 linearly spaced values between 0 and 10\n",
    "xValues = np.linspace(0,10,50)\n",
    "yValues = func(xValues,aParam,bParam)\n",
    "\n",
    "# 95% confidence interval for the data\n",
    "dely = result.eval_uncertainty(x=xValues,sigma=0.95)\n",
    "\n",
    "# Create a figure and plot the data\n",
    "fix, ax = plt.subplots()\n",
    "ax.plot(xValues,yValues,label='fit',color='r')\n",
    "ax.fill_between(xValues, yValues-dely, yValues+dely, color=\"gray\", alpha=0.5)\n",
    "ax.scatter(data[\"x\"],data['y'],label=\"data\")\n",
    "\n",
    "ax.set(xlabel=\"Y label (mol/L)\")\n",
    "ax.set(ylabel=\"X label\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted fit\n",
    "The data we used in the previous example also contained the uncertainty of the value. We can use that information to provide a larger weight to the values with the smaller uncertainty, and then refit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"../../miscData/LB.csv\")\n",
    "data.columns = (\"x\",\"y\",\"e\")\n",
    "# For clarity we put the values in the dataframe into variables\n",
    "x = data[\"x\"]\n",
    "y = data[\"y\"]\n",
    "e = data[\"e\"]\n",
    "\n",
    "# Define the function to be used in the fitting\n",
    "def func(x,a,b):\n",
    "    y = a*x + b\n",
    "    return y\n",
    "\n",
    "# Create the lmfit model\n",
    "lmodel = Model(func)\n",
    "\n",
    "# Initialise the function parameters to be optimised\n",
    "params = lmodel.make_params(a=1,b=1)\n",
    "\n",
    "# Fit the data:\n",
    "#   first argument is y\n",
    "#   second argument are the fitting parameters\n",
    "#   third argument is x\n",
    "result = lmodel.fit(y,params,x=x,weights=1/e)\n",
    "\n",
    "# Print the summary of the result\n",
    "print(result.fit_report())\n",
    "\n",
    "# Goodness of the fit\n",
    "fig = result.plot()\n",
    "ax = fig.gca()\n",
    "ax.set(xlabel=\"Y label\")\n",
    "ax.set(ylabel=\"X label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# $\\LaTeX$ for Greek Letters\n",
    "\n",
    "Here below you can find the greek letters and the corresponding $\\LaTeX$ commands to include them in your equations.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\verb| Lowercase letters| &\\qquad\\qquad& \\verb| Uppercase letters| \\\\\n",
    "\\verb|\\alpha      | \\alpha   &\\qquad\\qquad& \\verb|  A         |  A      \\\\\n",
    "\\verb|\\beta       | \\beta    &\\qquad\\qquad& \\verb|  B         |  B      \\\\\n",
    "\\verb|\\gamma      | \\gamma   &\\qquad\\qquad& \\verb| \\Gamma     | \\Gamma  \\\\\n",
    "\\verb|\\delta      | \\delta   &\\qquad\\qquad& \\verb| \\Delta     | \\Delta  \\\\\n",
    "\\verb|\\epsilon    | \\epsilon &\\qquad\\qquad& \\verb|  E         |  E      \\\\\n",
    "\\verb|\\zeta       | \\zeta    &\\qquad\\qquad& \\verb|  Z         |  Z      \\\\\n",
    "\\verb|\\eta        | \\eta     &\\qquad\\qquad& \\verb|  H         |  H      \\\\\n",
    "\\verb|\\theta      | \\theta   &\\qquad\\qquad& \\verb| \\Theta     | \\Theta  \\\\\n",
    "\\verb|\\iota       | \\iota    &\\qquad\\qquad& \\verb|  I         |  I      \\\\\n",
    "\\verb|\\kappa      | \\kappa   &\\qquad\\qquad& \\verb|  K         |  K      \\\\\n",
    "\\verb|\\lambda     | \\lambda  &\\qquad\\qquad& \\verb| \\Lambda    | \\Lambda \\\\\n",
    "\\verb|\\mu         | \\mu      &\\qquad\\qquad& \\verb|  M         |  M      \\\\\n",
    "\\verb|\\nu         | \\nu      &\\qquad\\qquad& \\verb|  N         |  N      \\\\\n",
    "\\verb|\\xi         | \\xi      &\\qquad\\qquad& \\verb| \\Xi        | \\Xi     \\\\\n",
    "\\verb|omicron     | o        &\\qquad\\qquad& \\verb|  O         |  O      \\\\\n",
    "\\verb|\\pi         | \\pi      &\\qquad\\qquad& \\verb| \\Pi        | \\Pi     \\\\\n",
    "\\verb|\\rho        | \\rho     &\\qquad\\qquad& \\verb|  P         |  P      \\\\\n",
    "\\verb|\\sigma      | \\sigma   &\\qquad\\qquad& \\verb| \\Sigma     | \\Sigma  \\\\\n",
    "\\verb|\\tau        | \\tau     &\\qquad\\qquad& \\verb|  T         |  T      \\\\\n",
    "\\verb|\\upsilon    | \\upsilon &\\qquad\\qquad& \\verb| \\Upsilon   | \\Upsilon\\\\\n",
    "\\verb|\\phi        | \\phi     &\\qquad\\qquad& \\verb| \\Phi       | \\Phi    \\\\\n",
    "\\verb|\\chi        | \\chi     &\\qquad\\qquad& \\verb|  X         |  X      \\\\\n",
    "\\verb|\\psi        | \\psi     &\\qquad\\qquad& \\verb| \\Psi       | \\Psi    \\\\\n",
    "\\verb|\\omega      | \\omega   &\\qquad\\qquad& \\verb| \\Omega     | \\Omega  \n",
    "\\end{eqnarray*}\n",
    "\n",
    "These are the $\\LaTeX$ commands for the most commonly used mathematical symbols\n",
    "\\begin{eqnarray*}\n",
    "\\verb|\\frac{a}{b}| &\\qquad\\qquad& \\frac{a}{b} \\\\\n",
    "\\verb+\\sum_{a}^{b}+ &\\qquad\\qquad& \\sum_{a}^{b} \\\\\n",
    "\\verb+\\prod_{a}^{b}+ &\\qquad\\qquad& \\prod_{a}^{b} \\\\\n",
    "\\verb+\\int_{a}^{b}+ &\\qquad\\qquad& \\int_{a}^{b} \\\\\n",
    "\\end{eqnarray*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e535d220b488de5ea91b9cc2b59c2f114a95afd4d79a4d51c41214c2f625fe35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
